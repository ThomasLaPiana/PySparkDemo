{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'published': 'Sat, 09 Apr 2016 18:15:19 +0000', 'title_detail': {'language': None, 'type': 'text/plain', 'value': 'Mounting data suggest antibacterial soaps do more harm than good', 'base': 'http://feeds.arstechnica.com/arstechnica/index?format=xml'}, 'summary_detail': {'language': None, 'type': 'text/html', 'value': 'Few pros, but cons include upped risk of infection, microbiome changes, drug resistance.', 'base': 'http://feeds.arstechnica.com/arstechnica/index?format=xml'}, 'author_detail': {'name': 'Beth Mole'}, 'authors': [{'name': 'Beth Mole'}], 'tags': [{'label': None, 'scheme': None, 'term': 'Scientific Method'}, {'label': None, 'scheme': None, 'term': 'antibacterial soap'}, {'label': None, 'scheme': None, 'term': 'antibiotic resistance'}, {'label': None, 'scheme': None, 'term': 'triclosan'}], 'id': 'http://arstechnica.com/?p=862109', 'author': 'Beth Mole', 'link': 'http://arstechnica.com/science/2016/04/mounting-data-suggest-antibacterial-soaps-do-more-harm-than-good/', 'title': 'Mounting data suggest antibacterial soaps do more harm than good', 'published_parsed': time.struct_time(tm_year=2016, tm_mon=4, tm_mday=9, tm_hour=18, tm_min=15, tm_sec=19, tm_wday=5, tm_yday=100, tm_isdst=0), 'guidislink': False, 'summary': 'Few pros, but cons include upped risk of infection, microbiome changes, drug resistance.', 'links': [{'href': 'http://arstechnica.com/science/2016/04/mounting-data-suggest-antibacterial-soaps-do-more-harm-than-good/', 'rel': 'alternate', 'type': 'text/html'}], 'content': [{'language': None, 'type': 'text/html', 'value': '<div id=\"rss-wrap\">\\n<div>\\n      <img src=\"http://cdn.arstechnica.net/wp-content/uploads/2016/04/6299070668_05a931ed1a_z-640x427.jpg\"><p class=\"caption\" style=\"font-size:0.8em\">(credit: <a rel=\"nofollow\" class=\"caption-link\" href=\"https://www.flickr.com/photos/ugacommunications/6299070668/in/photolist-aACofG-4zXyi9-qN558K-dX7RzB-ackzaf-8P3yXu-85nniy-7a2zdP-DTPN5-5jPVhh-4xAahK-chjPMq-6iVKB8-dV6peL-uvYDU-6XowFV-JmkMK-5kHErV-ahBLms-47qkAp-7FVoWK-9gYYoL-4q6kK1-3nXp9T-4N1Dor-6Kfih5-ahFp5D-cZ4g5J-8xhMyd-6mPRJh-q8zhZs-EJSxHM-fezTyM-amPzB1-nGiHD8-7vqAkR-bUPeH9-gd85Sc-c1aBaQ-d4F6bU-cHsmMS-5WNByC-dQYqpt-sgRzY6-3b32DR-jT3ru-aqrEf8-oNH1U2-N8qwY-bWrNpR\">UGA</a>)</p>\\n</div>\\n\\n\\n\\n\\n\\n<div><a name=\"page-1\"></a></div>\\n<p>Whether you’re coming home from an airport fluttering with international germs, a daycare full of sticky-fingered toddlers, or just a grimy office building, scrubbing your hands with bacteria-busting soap seems like a great idea. But the data that have washed up on the cleansers in recent years suggest that they actually do more harm than good—for you, those around you, and the environment.</p>\\n<p>Scientists report that common antibacterial compounds found in those soaps, namely triclosan and triclocarban, may increase the risk of infections, alter the gut microbiome, and spur bacteria to become resistant to prescription antibiotics. Meanwhile, proof of the soaps’ benefits is slim.</p>\\n<p>There are specific circumstances in which those antimicrobials can be useful, civil engineer Patrick McNamara of Marquette University in Milwaukee told Ars. Triclosan, for instance, may be useful to doctors scrubbing for minutes at a time before a surgery or for hospital patients who can’t necessarily scrub with soap but could soak in a chemical bath. Triclosan and triclocarban do kill off bacteria during long washes. But most people only clean their hands for a few seconds. “There’s evidence that there is no improvement with using soaps that have these chemicals relative to washing your hands under warm water for 30 seconds with soaps without these chemicals,” he said.</p>\\n</div><p><a href=\"http://arstechnica.com/science/2016/04/mounting-data-suggest-antibacterial-soaps-do-more-harm-than-good/#p3\">Read 15 remaining paragraphs</a> | <a href=\"http://arstechnica.com/science/2016/04/mounting-data-suggest-antibacterial-soaps-do-more-harm-than-good/?comments=1\">Comments</a></p><div class=\"feedflare\">\\n<a href=\"http://feeds.arstechnica.com/~ff/arstechnica/index?a=63jlFQFnn-M:iZtBX7zMt-E:V_sGLiPBpWU\"><img src=\"http://feeds.feedburner.com/~ff/arstechnica/index?i=63jlFQFnn-M:iZtBX7zMt-E:V_sGLiPBpWU\" border=\"0\"></img></a> <a href=\"http://feeds.arstechnica.com/~ff/arstechnica/index?a=63jlFQFnn-M:iZtBX7zMt-E:F7zBnMyn0Lo\"><img src=\"http://feeds.feedburner.com/~ff/arstechnica/index?i=63jlFQFnn-M:iZtBX7zMt-E:F7zBnMyn0Lo\" border=\"0\"></img></a> <a href=\"http://feeds.arstechnica.com/~ff/arstechnica/index?a=63jlFQFnn-M:iZtBX7zMt-E:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/arstechnica/index?d=qj6IDK7rITs\" border=\"0\"></img></a> <a href=\"http://feeds.arstechnica.com/~ff/arstechnica/index?a=63jlFQFnn-M:iZtBX7zMt-E:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/arstechnica/index?d=yIl2AUoC8zA\" border=\"0\"></img></a>\\n</div>', 'base': 'http://feeds.arstechnica.com/arstechnica/index?format=xml'}]}\n"
     ]
    }
   ],
   "source": [
    "####################################Imports and Initial Feed Rip###################################################\n",
    "import feedparser as fp, pandas as pd, datetime, time, string, requests as r\n",
    "from bs4 import BeautifulSoup \n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "rss_feed = 'http://feeds.arstechnica.com/arstechnica/index?format=xml'\n",
    "company_info_df = pd.read_csv('Target_Company.csv')\n",
    "company_name = company_info_df.iloc[0]['Company Name']\n",
    "\n",
    "\n",
    "#Get the News Feed and Slice it to only include the news items\n",
    "feed = fp.parse(rss_feed)\n",
    "feed_items = feed['items']\n",
    "print(feed_items[0])\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c00ed05292e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Create the Full Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mfull_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_news_dataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#Create the filtered dataframe, containing only rows that have company_name in the title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c00ed05292e2>\u001b[0m in \u001b[0;36mdataframe_cleaner\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeing_cleaned_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeing_cleaned_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#being_cleaned_df['content'][i] = soup.get_text()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/thomas/anaconda3/lib/python3.5/site-packages/beautifulsoup4-4.4.1-py3.5.egg/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m              self.builder.prepare_markup(\n\u001b[1;32m--> 212\u001b[1;33m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[0;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/thomas/anaconda3/lib/python3.5/site-packages/beautifulsoup4-4.4.1-py3.5.egg/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[1;34m(self, markup, user_specified_encoding, document_declared_encoding, exclude_encodings)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n\u001b[1;32m--> 154\u001b[1;33m                                exclude_encodings=exclude_encodings)\n\u001b[0m\u001b[0;32m    155\u001b[0m         yield (dammit.markup, dammit.original_encoding,\n\u001b[0;32m    156\u001b[0m                \u001b[0mdammit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_html_encoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/thomas/anaconda3/lib/python3.5/site-packages/beautifulsoup4-4.4.1-py3.5.egg/bs4/dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/thomas/anaconda3/lib/python3.5/site-packages/beautifulsoup4-4.4.1-py3.5.egg/bs4/dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_encoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             self.declared_encoding = self.find_declared_encoding(\n\u001b[1;32m--> 256\u001b[1;33m                 self.markup, self.is_html)\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/thomas/anaconda3/lib/python3.5/site-packages/beautifulsoup4-4.4.1-py3.5.egg/bs4/dammit.py\u001b[0m in \u001b[0;36mfind_declared_encoding\u001b[1;34m(cls, markup, is_html, search_entire_document)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mdeclared_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mdeclared_encoding_match\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxml_encoding_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxml_endpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdeclared_encoding_match\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_html\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mdeclared_encoding_match\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml_meta_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhtml_endpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#################################Create the Cleaned News Items DataFrame###########################################\n",
    "\n",
    "#Import the raw data into a DataFrame\n",
    "raw_news_dataframe = pd.DataFrame(feed_items)\n",
    "\n",
    "#Clean the DataFrame\n",
    "def dataframe_cleaner(dataframe): \n",
    "    being_cleaned_df = pd.DataFrame(raw_news_dataframe[['author','id','title','content']])\n",
    "    \n",
    "    #for i in range(0,len(being_cleaned_df['content'])):\n",
    "        #soup = BeautifulSoup(being_cleaned_df['content'][i],'html.parser')\n",
    "        #print(soup)\n",
    "        #being_cleaned_df['content'][i] = soup.get_text()\n",
    "    \n",
    "    return being_cleaned_df\n",
    "    \n",
    "#Create the Full Dataframe    \n",
    "full_df = dataframe_cleaner(raw_news_dataframe)\n",
    "\n",
    "#Create the filtered dataframe, containing only rows that have company_name in the title\n",
    "company_in_title_list = list(map(lambda x: company_name in x,full_df['title']))\n",
    "news_df = full_df[company_in_title_list].reset_index(drop=True)\n",
    "news_df.head()\n",
    "\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>word</th>\n",
       "      <th>position</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>adj</td>\n",
       "      <td>n</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type       word position stemmed  polarity\n",
       "0   0.5  abandoned      adj       n        -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in the Sentiment DataFrame\n",
    "sentiment_df = pd.read_csv('../Sentiment_Dictionary/sentiment_DataFrame.csv')\n",
    "sentiment_df = sentiment_df.ix[:,'type':]\n",
    "sentiment_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 1.54046630859375\n"
     ]
    }
   ],
   "source": [
    "##########################Perform Sentiment Analysis on the Title and Body of the Articles#########################\n",
    "def title_analysis(title):\n",
    "    tokenized_title = dict.fromkeys(title.split(' '))                          \n",
    "                                \n",
    "    for key in tokenized_title:\n",
    "        tokenized_title[key] = [sentiment_df['type'][i] * sentiment_df['polarity'][i]\n",
    "                               for i in range(0,len(sentiment_df.index)) \n",
    "                               if key.lower() == sentiment_df['word'][i]]\n",
    "    \n",
    "    total_sentiment = sum([item for sublist in list(tokenized_title.values()) for item in sublist])\n",
    "    normalized_sentiment = total_sentiment / len(tokenized_title)\n",
    "    \n",
    "    return normalized_sentiment\n",
    "\n",
    "def content_analysis(content):\n",
    "    tokenized_content = dict.fromkeys(content.split(' '))\n",
    "    \n",
    "    for key in tokenized_content:\n",
    "        tokenized_content[key] = [sentiment_df['type'][i] * sentiment_df['polarity'][i]\n",
    "                               for i in range(0,len(sentiment_df.index)) \n",
    "                               if key.lower() == sentiment_df['word'][i]]\n",
    "    \n",
    "    total_sentiment = sum([item for sublist in list(tokenized_content.values()) for item in sublist])\n",
    "    normalized_sentiment = total_sentiment / len(tokenized_content)\n",
    "    return normalized_sentiment\n",
    "\n",
    "def sentiment_analysis(dataframe):\n",
    "    analyzing_df = dataframe\n",
    "    analyzing_df['normalized_title_sentiment'] = list(map(lambda x: title_analysis(x),analyzing_df['title']))\n",
    "    analyzing_df['normalized_content_sentiment'] = list(map(lambda x: content_analysis(x),analyzing_df['content']))\n",
    "    return analyzing_df\n",
    "\n",
    "start_time = time.time()\n",
    "analyzed_articles_df = sentiment_analysis(news_df)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Elapsed Time: ' + str(elapsed_time))\n",
    "analyzed_articles_df.head(1)\n",
    "analyzed_articles_df.to_csv('ArsTechnicaSentiment.csv')\n",
    "###################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
